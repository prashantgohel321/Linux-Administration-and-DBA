# PGSQL Statistics System – How a DBA Actually Uses It

<br>
<br>

- [PGSQL Statistics System – How a DBA Actually Uses It](#pgsql-statistics-system--how-a-dba-actually-uses-it)
  - [Scenario Overview](#scenario-overview)
  - [Step 1: What the Statistics System Really Is](#step-1-what-the-statistics-system-really-is)
  - [Step 2: A User Says “The Database Is Slow”](#step-2-a-user-says-the-database-is-slow)
  - [Step 3: Understanding Waiting vs Working](#step-3-understanding-waiting-vs-working)
  - [Step 4: Why Queries Appear Stuck](#step-4-why-queries-appear-stuck)
  - [Step 5: Table-Level Behavior](#step-5-table-level-behavior)
  - [Step 6: Index Reality Check](#step-6-index-reality-check)
  - [Step 7: Memory vs Disk Truth](#step-7-memory-vs-disk-truth)
  - [Step 8: Database-Wide Health](#step-8-database-wide-health)
  - [Step 9: WAL and Checkpoint Pressure](#step-9-wal-and-checkpoint-pressure)
  - [Step 10: Background Writer and Checkpoints](#step-10-background-writer-and-checkpoints)
  - [Step 11: Replication Visibility](#step-11-replication-visibility)
  - [Step 12: Archiving and Recovery Safety](#step-12-archiving-and-recovery-safety)
  - [Step 13: Statistics Are Not Instant](#step-13-statistics-are-not-instant)
  - [Step 14: Resetting Statistics Carefully](#step-14-resetting-statistics-carefully)
  - [Final Understanding Through This Flow](#final-understanding-through-this-flow)


<br>
<br>

## Scenario Overview

- A PGSQL server is running in production. Users complain that queries are slow, replication seems delayed, disk I/O spikes randomly, and sometimes sessions just hang. Nothing is obviously broken, but performance feels unstable.

- At this point, a DBA does not guess. A DBA looks at PGSQL statistics.

---

<br>
<br>

## Step 1: What the Statistics System Really Is

- The statistics system is how PGSQL watches itself.
- Every backend process keeps track of 
  - *what it is doing while running a query like*
  - *How many rows it reads.* 
  - *Whether data comes from disk or memory.* 
  - *And if it is waitinig for a lock*

- All of this information is stored in shared memory and shown through `pg_stat_*` views. These view dont show settings. They show what is actually happening.
- So when the database feels slow or stuck, the answer is usually already there.

---

<br>
<br>

## Step 2: A User Says “The Database Is Slow”

- The first question is always: what is running right now?

A DBA checks:

```sql
SELECT pid, state, wait_event_type, wait_event, query
FROM pg_stat_activity
WHERE state <> 'idle'; -- <> means "not equals to".
```

- This view shows one process for each server process. It quickly tells you who is connected, which queries are running, whether they are actively executing or stuck waiting, and exaclty what they are waiting for.
- Think of `pg_stat_activity` as the live heartbeat of PGSQL. It shows all current connections, what each one is doing, and why something might feel slow.

---

<br>
<br>

## Step 3: Understanding Waiting vs Working

- A backend can be **`active`** but still blocked.
- If `state` is **`active`** and `wait_event` is **`NULL`**, the query is running on CPU.
- If `wait_event_type` is **`Lock`**, the query is blocked by another transaction.
- If it is **`IO`**, PGSQL is waiting for disk.
- If it is **`LWLock`**, PGSQL is waiting for an internal shared-memory structure.
- This distinction tells a DBA whether the problem is CPU, disk, locks, or internal contention.

<br>
<details>
<summary><mark><b>wait_event_type and wait_event</b></mark></summary>
<br>

**`wait_event_type`**: This tells what category of thing a backend is waiting for.

- Common values:
  - **`Lock:`** waiting for a database lock held by another transaction.
  - **`LWLock:`** waiting for an internal lightweight pgsql lock (shared memory structures)
  - **`IO:`** waiting for disk read / write.
  - **`Client:`** waiting for cleint input / output.
  - **`Timeout:`** waiting for a timeout to expire
  - **`Activity:`** waiting on internal background activity
  - **`Extension:`** waiting inside an extension
  - **`NULL:`** not waiting, queyr is actively runnning on CPU.

**`wait_event`**: This gives the exact reason inside that category.
- Examples:
  - **`Lock`** -> **`transactionid`**, **`relation`**, **`tuple`**
  - **`LWLock`** -> **`WALWriteLock`**, **`BufferContent`**
  - **`IO`** -> **`DataFileRead`**, **`DataFileWrite`**
  - **`Client`** -> **`ClientRead`**, **`ClientWrite`**
  - **`Timeout`** -> **`StatementTimeout`**
  - **`NULL`** -> means the backend is working, not blocked.

</details>
<br>

<br>
<details>
<summary><mark><b>Explained with scenarios</b></mark></summary>
<br>

- A PGSQL backend can look active, but that doesnt mean it is actually doing work. The key is to look at **`state`**, **`wait_event_type`**, and **`wait_event`** together.

- We can check what a backend (session) is doing right now by looking at `pg_stat_activity`. The key columns are:
  - `state`: Usually `active` (running a query) or `idle`.
  - `wait_event_type` and `wait_event`: Tell us if the backend is working or waiting.

<br>
<br>

**1. Working on CPU (Query is actively running)**
- **`state = active`**
- **`wait_event_type = NULL`** (wait_event is NULL)
  - The query is using CPU right now - calculating, sorting etc...

**Example:**
```text
state           | active
query           | SELECT sum(amount) FROM orders WHERE date > '2025-01-01';
wait_event_type | NULL
wait_event      | NULL
```

> This query is busy crunching numbers on CPU. No Blocking.

<br>
<br>

**2. Waiting for a lock (Blocked by anoter session)**
- **`state = active`**
- **`wait_event_type = Lock`**
- **`wait_event`** = **`transactionid`** or **`relation`** or **`advisory`** etc.

**Example:**
```text
state           | active
query           | UPDATE users SET balance = balance - 100 WHERE id = 5;
wait_event_type | Lock
wait_event      | transactionid
```

> This UPDATE is waiting because another session has an uncommited transaction on the same row / table.

<br>
<br>

**3. Waiting for Disk I/O**
- **`state = active`**
- **`wait_event_type = IO`**
- Common **`wait_event`**: **`DataFileRead`**, **`DataFileWrite`**

**Example:**
```text
state           | active
query           | SELECT * FROM big_table WHERE id = 999999;
wait_event_type | IO
wait_event      | DataFileRead
```

> PGSQL is waiting for the disk to read a page (maybe not in memory / cache).

<br>
<br>

**4. Waiting for internal LWLock (Lightweight Lock - Internal Contention**
- **`state = active`**
-**` wait_event_type = LWLock`**
- Examples: **`buffer_content`**, **`lock_manager`**, **`WALWrite`**

**Examples:**
```text
state           | active
query           | INSERT INTO logs VALUES (...);
wait_event_type | LWLock
wait_event      | WALWrite
```

> Many sessions inserting at once - waiting to write to WAL safely.

<br>
<br>

**Why this everything matters?**
- If the backend have **`wait_event_type`** = **`NULL`** -> Proble is CPU (need better queries / indexes).
- Lots of **`Lock`** waits -> Lock contention (Long transactions, missing indexes).
- Lots of **`IO`** waits -> Disk slow (Need more RAM, faster storage, better caching).
- Lots of **`LWLock`** -> Interanal Contention (High concurrency, tune shared_buffers \ work_mem).


</details>
<br>

---

<br>
<br>

## Step 4: Why Queries Appear Stuck

- When sessions show `idle in transaction`, it means the client started a transaction and forgot to finish it.
- These sessions hold locks and block vacuum.
- **`pg_stat_activity`** exposes this clearly through `state` and `xact_start`.
- Long-running idle transactions are one of the most common production killers.

<br>
<details>
<summary><mark><b>Explained with scenarios!</b></mark></summary>
<br>

- We often see queries that look "stuck" in production - but the real rpoblem is idle in transaction sessions.

**What it means:**
- The client stated a transaction (BEGIN) but never finished it (**`COMMIT`** or **`ROLLBACK`**).
- The session is **`idle`** (not running any query right now), but it still holds locks and blocks other work.

**How we can spot it in pg_stat_activity:**
- **`state = idle in transaction`**
- **`xact_start`** shows when the transaction began (if it is old -> danger)

**Example:**
- When we run this query and see:
```bash
pid                | 12345
state              | idle in transaction
query              | (empty – no active query)
xact_start         | 2025-12-23 09:15:00
backend_start      | 2025-12-23 09:00:00
```

> This session started a transaction 45 mins ago and forgot to COMMIT or ROLLBACK. It is holding locks on rows or tables it touched earlier.

**What happens becuase of this?**
- Other sessions trying to UPDATE or DELETE the same rows get blocked -> appear stuck.
- VACCUM cannot clean old dead rows -> table bloat grows.
- AUTOVACCUM gets blocked -> more bloat and wraparound risk.

**Common Real-life cases:**
- Web app forgets to commit after an error.
- Developer runs BEGIN in psql and walks away.
- Long script starts transaction but craseshes before COMMIT.

**Run this and find them:**
```bash
SELECT pid, state, xact_start, query
FROM pg_stat_activity
WHERE state = 'idle in transaction'
ORDER BY xact_start;

# here xact means transaction and xact_start shows the time when the current transaction began
# it helps to identify long-running or stuck queries.

# backend_start shows the time when the backend (client connection) was started.
```

If you see old ones -> kill them safely:
```sql
SELECT pg_terminate_backend(12345);
```

**Difference between `idle` and `idle in transaction`:**
- **`idle`**: The session is connected but doing nothing — no query running and no open transaction.
- **`idle in transaction:`** The session is not running a query, but a transaction is still open (`BEGIN` done, `COMMIT` / `ROLLBACK` not yet)
- **Why it matters?** `idle in transaction` is harmful - it holds locks and prevents vacuum, while `idle` is harmless.

**Summary:** Idle in transaction = forgottern open transaction -> holds locks, blocks vaccum, kills performance. Long ones are one of the top production killers.


</details>
<br>

---

<br>
<br>

## Step 5: Table-Level Behavior

Now the DBA asks: which tables are actually being touched?

```sql
SELECT relname, seq_scan, idx_scan, n_dead_tup
FROM pg_stat_user_tables
ORDER BY seq_scan DESC;
```

- This shows how tables are accessed.
- High sequential scans on large tables often indicate missing or unused indexes.
- High dead tuples indicate vacuum pressure.
- This view explains *why* disk usage grows and queries slow down over time.

<br>
<details>
<summary><mark><b>Explained with scenarios!</b></mark></summary>
<br>

**What is pg_stat_user_tables?**
- a system statistics view that shows activity and health info for user-created tables.
- It tells which tables are being used,
  - updated
  - vaccumed, or
  - bloated
  - things like row inserts, updates, deletes, and dead rows.

- When queries are slow or disk usage keeps growing, we need to look at the table-level activity. The best view for this is pg_stat_user_tables.

**Query to run:**
```sql
SELECT 
    relname AS table_name
    seq_scan,     -- Full table scan
    idx_scan,     -- Index scan
    n_tup_ins,    -- Rows inserted
    n_tup_upd,    -- Rows updated
    n_tup_del,    -- Rows deleted
    n_dead_tup    -- Dead Rows (need vaccum)
FROM pg_stat_user_tables
ORDER BY seq_scan DESC;
```

<br>
<br>

**Scenarios:**
**1. High `seq_scan` + Low `idx_scan` -> Missing or unused indexes:**

**Example:**
```text
table_name    | orders
seq_scan      | 15230
idx_scan      | 120
```
> This table is scaned fully 15k times but barely uses indexes -> queries are slow.
**Fix:** We should add proper indexes on `WHERE`, `JOIN`, or `ORDER BY` columns.

<br>
<br>

**2. High `n_dead_tup` -> Heavy Vaccum Pressure:**

**Example:**
```text
table_name    | logs
seq_scan      | 500
n_dead_tup    | 2,850,000
```

> 2.8 million dead rows waiting to be cleaned -> table bloat growing fast.

**Why:** Lots of `DELETE` / `UPDATE`, but `VACUUM` not keeping up.
**Fix:** Run `VACUUM (VERBOSE) logs;` or tune `autovacuum` for this table.

<br>
<br>

**3. High `n_tup_upd` + High `n_dead_tup` -> Update- heavy table:**

**Example:**
```text
table_name    | sessions
n_tup_upd     | 10,500,000
n_dead_tup    | 8,200,000
```

> Every `update` created a new row version -> huge bloat.

**Common in:** session tables, audit logs, status tracking.
**Fix:** More frequent VACUUM or consider table partitioning.

<br>
<br>

**4. High `idx_scan` + Low `seq_scan` -> Indexes are working well:**

**Example:**
```text
table_name    | customers
seq_scan      | 50
idx_scan      | 985,000
```

> Almost all queries use indexes -> healthy access pattern.

<br>
<br>

**5. High inserts + Growing size but low dead tuples -> Fast growing tabe:**

**Example:**
```text
table_name    | events
n_tup_ins     | 50,000,000
n_dead_tup    | 120,000
```

> Table is growing fast from inserts. Bloat is low (good vacuum).

**Action:** Monitor disk space, consider partitioning later.

<br>
<br>

**Run this query regularly:**
```sql
SELECT relname, seq_scan, idx_scan, n_tup_ins + n_tup_upd + n_tup_del AS changes, n_dead_tup
FROM pg_stat_user_tables
ORDER BY n_dead_tup DESC;
```

- Top rows with high `seq_scan` → candidates for new indexes.
- Top rows with high `n_dead_tup` → need VACUUM or autovacuum tuning.

**Summary:** `pg_stat_user_tables` shows us exactly which tables are misbehaving – missing indexes, bloat, or heavy write load. Check it when performance drops or disk fills up!



</details>
<br>

---

<br>
<br>

## Step 6: Index Reality Check

- Indexes existing does not mean indexes are used.

A DBA checks:

```sql
SELECT relname, indexrelname, idx_scan
FROM pg_stat_user_indexes
ORDER BY idx_scan ASC;
```

- Indexes with zero scans are unused.
- Unused indexes waste disk, slow writes, and confuse planners.
- `pg_stat_user_indexes` exposes index usefulness, not just existence.

<br>
<details>
<summary><mark><b>Explained with scenario!</b></mark></summary>
<br>

**What is pg_stat_user_indexes?**
- shows how indexes on  user tables are actually being used.
- It tells you which indexes pgsql is using,
  - how often they are scanned,
  - helps you to spot unused or unnecessary indexes.

- We know that having indexes is good, but existing indexes ≠ used indexes. 
- Many indexes just waste space and slow down writes without helping queries.
- The best waay tot check reality is `pg_stat_user_indexes`.

**Query to run:**
```sql
SELECT
    relname AS table_name,
    indexrelname AS index_name,
    idx_scan,         -- How many times this index was used
    idx_tup_read,     -- Tuples fetched by index
    idx_tup_fetch     -- Rows returned from table via index
FROM pg_stat_user_indexes
ORDER BY idx_scan ASC;
```

<br>
<br>

**All Possible Scenarios:**

**1. Zero or Very Low `idx_scan` -> Completely unused index:**
```text
table_name    | orders
index_name    | idx_orders_status_old
idx_scan      | 0
```

> This index has never been used (since last stats reset).

**Action:** Safe to DROP it - saves space and speeds up `INSERT` / `UPDATE` / `DELETE`.

<br>
<br>

**2. Low `idx_scan` on a large table -> Rarely used index:**
**Example:**
```text
table_name    | users
index_name    | idx_users_last_login
idx_scan      | 15
```

> Used only 15 times times (maybe an admin report runs monthly).

**Action:** Check if the query really needs it. If rare, consider dropping or keeping only if crucial.


<br>
<br>

**3. High `idx_scan` -> Actively used index:**
**Example:**
```text
table_name    | orders
index_name    | idx_orders_customer_date
idx_scan      | 1,250,000
```
> This index is heavily used -> queries are fast because of it.

**Action:** Keep it!

<br>
<br>

**4. Duplicate / Similar Indexes -> Redundant Indexes**
**Example:**
```text
table_name    | products
index_name    | idx_products_category
idx_scan      | 850,000
index_name    | idx_products_category_active
idx_scan      | 12
```

> Second index includes extra column but barely used -> probably redundant.

**Action:** Check if queries can use the first one. Drop the unused one.

<br>
<br>

**5. Index with high `idx_scan` but low `idx_tup_fetch` -> index used but not selective:**
**Example:**
```text
table_name    | logs
index_name    | idx_logs_level
idx_scan      | 500,000
idx_tup_fetch | 490,000
```

> Index scanned a lot but returns almost all rows -> not helpful

**Action:** Consider removing or making it more selective (combine with another column).

<br>
<br>

**Run this query regularly:**
```sql
SELECT relname, indexrelname, idx_scan
FROM pg_stat_user_indexes
WHERE idx_scan < 100    -- find rarely used ones
ORDER BY idx_scan ASC;
```

- Indexes with 0 scans → drop immediately.
- Low scans → investigate queries.
- Unused indexes slow down writes (INSERT/UPDATE/DELETE) and waste disk.

**Summary:** `pg_stat_user_indexes` shows the truth about index usage – many indexes look useful but are dead weight. Check `idx_scan` and drop the zeros – your writes will thank you!

</details>
<br>

<br>
<details>
<summary><mark><b>Why PostgreSQL Does Sequential Scan Even With a Perfect Index?</b></mark></summary>
<br>

**1. Outdated Statistics:**
- The planner uses old table stats and thinks your condition matches way more (or fewer) rows than it really does.
- **Example:** Tables has 10 million rows, Your `WHERE status = 'active'` matches only 100 rows, but last `ANALYZE` was weeks ago -> Planner thinks it matches 2 million -> chooses `seq_scan`.
- **Fix:** Run `ANALYZE table_name;` (or `VACUUM ANALYZE;`). This updates ststs and usually fixes it.

**2. Low Selectivity (Condition Matches Too Many Rows)**
- Even with an index, if the `WHERE` clause returns 20-30%+ of the table, `seq scan` is often faster (reading pages sequentially is cheaper than random index lookups).
- **Example**: `WHERE country = 'USA'` and 40% of users are from USA → `seq scan` wins.
- **Fix**: Nothing wrong—it's the right choice. Add more conditions to make it selective.


**3. Table is very small:**
- For tiny tables (< few thousand rows), sequence scan is faster than index overhead.
- **Example:** A lookup table with 500 rows -> Planner always prefers sequence scans.

**4. Query uses non-indexable condition:**
- You put a function on the column or wrong data type.
- **Example:** Index on `created_date`, but query has `WHERE created_date::text LIKE '2025%'` -> Index cant be used.
- **Fix:** Rewrite query (`WHERE created_date >= '2025-01-01' AND created_date < '2026-01-01'`) or created expression index.

</details>
<br>

---

<br>
<br>

## Step 7: Memory vs Disk Truth

The database feels slow. Is it disk or memory?

```sql
SELECT relname, heap_blks_read, heap_blks_hit
FROM pg_statio_user_tables;
```

- If hits are much higher than reads, data is served from shared buffers.
- If reads dominate, PGSQL is hitting disk.
- This single view tells whether tuning memory or storage will help.

<br>
<details>
<summary><mark><b>Explained with scenarios!</b></mark></summary>
<br>

**What is pg_statio_user_tables?**
- a system I/O statistics view for used tables.
- shows how much table data is beindg read from memory vs disk, helping you to understand cache usage and disk I/O pressure.
- When the DB feels slow, we need to know the real culprit - memory (not enough cache) or disk (too many physical reads).
- The best view to check this is `pg_statio_user_tables`.

**Query to run:**
```sql
SELECT
    relname AS tables_name,
    heap_blks_read,      -- Blocks read from disk
    heap_blks_hit,       -- Blocks found in memory (shared_bufferes / cache)
    round(100.0 * heap_blks_hit / (heap_blks_hit + heap_blks_read + 1), 2) AS cache_hit_ratio
FROM pg_statio_user_tables
ORDER BY heap_blks_read DESC;
```

**All possible scenarios:**

**1. High `heap_blks_hit` >> `heap_blks_read` -> Good cache hit (mostly memory)**
**Example:**
```text
table_name       | customers
heap_blks_hit    | 9,850,000
heap_blks_read   | 150,000
cache_hit_ratio  | 98.50
```

> 98% data served from memory -> healthy

**Action:** No memory tunning needed. Problem is likely CPU or queries, not I/O.

<br>
<br>


**2. High `heap_blks_read` = or > `heap_blks_hit` -> Too many disk reads:**
**Example:**
```text
table_name       | orders_big
heap_blks_hit    | 2,100,000
heap_blks_read   | 4,800,000
cache_hit_ratio  | 30.43
```

> Only 30% cache hit -> hitting disk a lot -> slow queries

**Action:** Increase `shared_buffers`, add more RAM, or tuen queries to access less data.

<br>
<br>

**3. Very high reads on specific large tables -> Hot tables dont fit in memory:**
**Example:**
```text
table_name       | logs_archive
heap_blks_hit    | 50,000
heap_blks_read   | 3,200,000
cache_hit_ratio  | 1.54
```

> This huge tuple is scanned often but barely cached.

**Action:** Add better indexes (reduce scans), partition the table, or move old data to archive.

<br>
<br>

**4. Balanced But low overall hits -> System wide memory pressure:**
**Example:** (multiple tables show ~70-80% hit ratio)
```text
table_name    | sales_2025    | hit: 1.2M | read: 400k | ratio: 75%
table_name    | inventory     | hit: 800k | read: 300k | ratio: 72%
```

> Not terrible, but room for improvement across the board.
**Action:** Increase `shared_buffers` (aim for 90-95% cluster-wide), or check OS cache (effective_cache_size).

<br>
<br>

**5. Zero or low activity tables with some reads -> Cold data:**
**Example:**
```text
table_name      | old_reports
heap_blks_hit   | 5,000
heap_blks_read  | 80,000
```

> Rarely used table, reads only when accessed -> normal.

**Action:** No worry - expected for archive / cold tables.

<br>
<br>

**Quick cluster wide check:**
```sql
SHOW cache_hit_ratio;  

-- Or manual:

SELECT sum(heap_blks_hit) / (sum(heap_blks_hit) + sum(heap_blks_read)) * 100 AS overall_hit_ratio
FROM pg_statio_user_tables;
```

- 99%+ hit ratio → Memory is fine, look at queries/CPU.
- Below 95% → Add more RAM or increase shared_buffers.
- Below 90% → Urgent – disk I/O is killing performance.

**Summary**: `pg_statio_user_tables` reveals the truth: <mark><b>high</b></mark> `heap_blks_hit` = happy memory, <mark><b>high</b></mark> `heap_blks_read` = disk bottleneck. Check it before buying SSDs or adding RAM!

</details>
<br>


---

<br>
<br>

## Step 8: Database-Wide Health

A DBA zooms out to database level:

```sql
SELECT datname, xact_commit, xact_rollback, deadlocks, temp_files
FROM pg_stat_database;
```

- High rollbacks indicate application errors.
- Deadlocks indicate poor transaction design.
- Temp files indicate insufficient work_mem or bad query plans.
- This view summarizes how healthy each database is.

<br>
<details>
<summary><mark><b>Explained with scenarios!</b></mark></summary>
<br>

**What is pg_database?**
- shows activity and load per database, not per table or session.
- It tells you how busy each database is - number of connections,
  - commits,
  - rollbacks,
  - cache hits,
  - disk reads,
  - temp files,
  - and timinig info
- useful to spot which database is causing load problems.

- When table-level checks are not enough, we need to look at the whole db health.

**Query to run:**
```sql
SELECT 
  datname AS database_name,
  xact_commit,
  xact_rollback,
  deadloacks,
  temp_files,
  temp_bytes
FROM pg_stat_database
WHERE datname NOT IN ('template0', 'template1', 'postgres');
```

<br>
<br>

**All possible scenarios:**

**1. High `exact_rollback` compared to `xact_commit` -> Application error or explicit rollbacks:**
**Example:**
```text
datname       | myapp_db
xact_commit   | 1,250,000
xact_rollback | 480,000
```

> Almost 28% transactions fail -> too many errors!

**Commond Causes:** App exceptions not handled, duplicate key violations, constraint errors.
**Action:** Check application logs, add better error handling, or fix common conflicts.

<br>
<br>

**2. High `deadlocks` -> poor transaction design or lock contention:**
**Example:**
```text
datname       | payments_db
deadlocks     | 157
```
> 157 deadlocks since restart -> queries fighting over same rows in wrong order.
**Common Causes:** Multiple updates on same tables without consistent access order.
**Action:** Use consistent row access order, shorter transactions, or advisory locks.

<br>
<br>

**3. High `temp_files` and `temp_bytes` -> Queries spilling to disk (low work_mem or bad plans):**
**Example:**
```text
datname       | analytics_db
temp_files    | 8,420
temp_bytes    | 92,150,000,000  -- ~92 GB
```

> Queries writing huge temp files -> slow because of disk I/O.

**Common Causes:** Big sorts, hashes, aggregates without enough `work_mem`.
**Action:** Increae `work_mem` (per operation), rewrite queries, add indexes, or break queries into smaller parts.

<br>
<br>

**4. Very high `xact_commit` + low everything else -> Healthy database:**
**Example:**
```text
datname       | webshop_db
xact_commit   | 15,800,000
xact_rollback | 12,000
deadlocks     | 3
temp_files    | 45
```

> 99.9% commits succeed, almost no deadlocks / temp.

<br>
<br>

**5. High rollbacks + high temp_files -> combo problem (erros + memory pressure)**
**Example:**
```text
datname       | reporting_db
xact_rollback | 95,000
temp_files    | 12,300
```

> Failing complex queries spilling to disk -> retry loops or bad reports.

**Action:** Fix query errors first, then tune `work_mem`.


<br>
<br>


**Quick Cluster-wide check:**
```sql
SELECT datname, 
       round(100.0 * xact_rollback / (xact_commit + xact_rollback + 1), 2) AS rollback_pct,
       deadlocks, temp_files
FROM pg_stat_database;
```

- Rollback % > 5% → Investigate app errors.
- Deadlocks > 10-20 → Fix transaction patterns.
- Temp files > 1000 → Increase work_mem or optimize queries.

**Summary:** `pg_stat_database` gives us the overall health report – high rollbacks = app bugs, deadlocks = bad design, temp files = memory/query issues. Check it weekly to catch problems early

</details>
<br>

---

<br>
<br>

## Step 9: WAL and Checkpoint Pressure

- When disk spikes appear, WAL is a suspect.

```sql
SELECT wal_records, wal_bytes, wal_buffers_full
FROM pg_stat_wal;
```

- High WAL generation means heavy write workload.
- Frequent WAL buffer full events indicate WAL pressure.
- This explains fsync spikes and replication lag.


<br>
<details>
<summary><mark><b>Explained with scenarios!</b></mark></summary>
<br>

**What is pg_stat_wal?**
- shows WAL activity.
- It tells you how much WAL pgsql is generating and writing - usful to understand write load, replication pressure, and chekpoint / WAL tunning issues.
- When we see sudden disk I/O spikes, slow writes, or replication lag, WAL is often the hiddne culprit.

**Query to run:**
```sql
SELECT 
    wal_records,         -- Total WAL records generated
    wal_fpi,             -- Full page images written
    wal_bytes,           -- Total WAL data written (bytes)
    wal_buffers_full,    -- Times WAL buffer was full (throttled writes)
    wal_write_time,      -- Time spent writing WAL
    wal_sync_time        -- Time spent syncing WAL to disk
FROM pg_stat_wal;
```


<br>
<br>

**All possible scenarios:**
**1. High `wal_records` + High `wal_bytes` -> Heavy write workload**
**Example:**
```text
wal_records   | 85,420,000
wal_bytes     | 1,250,000,000,000  (~1.25 TB)
```

> Lots of `INSERT` / `UPDATE` / `DELETE` -> huge WAL generation

<br>
<br>

**2. High `wall_buffers_full` -> WAL buffer pressure (throttling writes)**
**Example:**
```text
wal_buffers_full | 12,450
wal_bytes        | 850,000,000,000
```

> WAL data is generated faster than it can be written -> backend slow down.

**Result:** fsync spikes, slow commits, disk busy.
**Action:** Increase `wal_buffers` (default 16mb -> try 64mb), use faster storage, or reduce write load.

<br>
<br>

**3. High `wal_sync_time` + Low `wal_write_time` -> slow disk sync (fsync bottleneck)**
**Example:**
```text
wal_write_time   | 850,000 ms
wal_sync_time    | 45,200,000 ms (~12 hours)
```

> Writing is fast, but syncing to disk is very slow → bad storage.
**Action:** Use SSD, set `wal_sync_method` = `open_sync` or `fdatasync`, check storage performance.

<br>
<br>

**4. High wal_fpi (Full Page Images) → Many checkpoint-related writes:**
**Example:**
```text
wal_fpi       | 28,500,000
wal_records   | 95,000,000
```

> ~30% of WAL is full page images → checkpoints too frequent or unclean pages.
**Action:** Increase `max_wal_size`, tune `checkpoint_timeout`, or increase `shared_buffers`.

<br>
<br>

**5. Low everything → Quiet WAL (healthy)**
**Example:**
```text
wal_records      | 120,000
wal_buffers_full | 0
wal_sync_time    | 45,000 ms
```

> Very low activity -> healthy system

<br>
<br>

**6. High WAL + Replication Lag -> WAL not being archived / streamed fast enough:**
**Exmample:**
- High `wal_bytes` + replicas lagging → WAL files piling up.
- **Action:** Check `archive_command` success, increase bandwidth, or tune `wal_sender_timeout`.


<br>
<br>

**Query to run:**
```sql
SELECT 
    pg_size_pretty(wal_bytes) AS wal_generated,
    wal_buffers_full,
    pg_size_pretty(pg_wal_lsn_diff(pg_current_wal_lsn(), '0/0')) AS current_wal_size
FROM pg_stat_wal;
```

- `wal_buffers_full` > 0 → Urgent: increase wal_buffers or get faster disk.
- High `wal_sync_time` → Storage is the bottleneck.
- High `wal_bytes` → Expected on write-heavy DBs; tune checkpoints.

**Summary:** `pg_stat_wal` reveals why writes are slow: high generation = heavy load, buffer full = memory pressure, high sync = slow disk. Check it whenever you see disk or commit lag spikes!

</details>
<br>

---

<br>
<br>

## Step 10: Background Writer and Checkpoints

A DBA checks whether backends are doing too much dirty work:

```sql
SELECT buffers_backend, buffers_clean, checkpoints_req
FROM pg_stat_bgwriter;
```

- High backend writes mean background writer is not keeping up.
- Frequent requested checkpoints indicate aggressive write patterns.
- This directly impacts latency.

<br>
<details>
<summary><mark><b>Explained with scenarios!</b></mark></summary>
<br>

**What is pg_stat_bgwriter?**
- shows what the backgroun writer and checkpointer are doing.
- It tells you how pgsql is flushing dirty pages to disk - whether work is done smoothly by backgroun processes or forced aggressively during checkpoints (which can degrade performance).
- When we notice high write latency, slow commits, or disk spikes, the background writer and checkpointer are key suspects.

**Query to run:**
```sql
SELECT 
    checkpoints_timed,      -- Scheduled checkpoints
    checkpoints_req,        -- Requested (forced) checkpoints
    buffers_checkpoint,     -- Pages written by checkpoints
    buffers_clean,          -- Pages written by background writer
    buffers_backend,        -- Pages written directly by backends
    buffers_backend_fsync   -- Times backends had to fsync themselves
FROM pg_stat_bgwriter;
```

<br>
<br>

**All possible scenarios:**

**1. High buffers_backend + High buffers_backend_fsync -> background writer failing badly**
**Example:**
```text
buffers_clean         | 1,250,000
buffers_backend       | 18,500,000
buffers_backend_fsync | 4,820
```

> Backends are writing most dirty pages themselves -> slow queries / commits.
**Causes:** Background writer too slow, too few dirty pages available, or very heavy write load.
**Action:** Increases bgwrite_lru_maxpages, increase bgwriter_delay, or add more RAM / shared_buffers.

<br>
<br>

**2. High checkpoints_req -> Too many forced / requested checkpoiunts:**
**Example:**
```text
checkpoints_timed | 480
checkpoints_req   | 3,250
```

> Most checkpoints are forced (because WAL filled up fast) -> write spikes every few minutes.

**Causes:** max_wal_size too low, very write-heavy workload.
**Action:** Increase max_wal_size and checkpoint_timeout -> smoother, less frequent checkpoints.

<br>
<br>

**3. High buffers_checkpoint + Low buffers_clean -> Checkpointer doing all the work:**
**Example:**
```text
buffers_checkpoint | 45,000,000
buffers_clean      | 2,100,000
buffers_backend    | 1,800,000
```

> Background writer barely helps -> checkpoints write huge bursts -> IO storms.

- **Action:** Tune background writer (higher bgwriter_lru_maxpages), increase shared_buffers.

<br>
<br>

**4. High buffers_clean + Low buffers_backend → Background writer doing its job well (healthy)**
**Example:**
```text
buffers_clean     | 28,000,000
buffers_backend   | 850,000
checkpoints_req   | 12
```

> Background writer cleans most pages → backends rarely write → smooth performance.

<br>
<br>

**5. Balanced but frequent checkpoints → Steady heavy write load**
**Example:**
```text
checkpoints_timed | 1,200
checkpoints_req   | 850
buffers_clean     | 15,000,000
buffers_backend   | 3,200,000
```

> Lots of writes, but background writer helps → manageable.

**Action**: Monitor, consider larger max_wal_size or faster storage.

<br>
<br>

```sql
SELECT 
    round(100.0 * buffers_backend / (buffers_checkpoint + buffers_clean + buffers_backend + 1), 2) AS backend_write_pct,
    checkpoints_req,
    buffers_backend_fsync
FROM pg_stat_bgwriter;
```

- `backend_write_pct` > 20% → Background writer not keeping up.
- `checkpoints_req` >> checkpoints_timed → WAL fills too fast.

- High `buffers_backend` → backends suffer → tune background writer.
- High `checkpoints_req` → increase `max_wal_size`.
- High `buffers_backend_fsync` → storage too slow.

**Summary:** `pg_stat_bgwriter` shows who writes dirty pages: high `buffers_backend` = backends struggling (slow), high `checkpoints_req` = aggressive checkpoints (spiky). Tune them for low latency!

</details>
<br>

---

<br>
<br>

## Step 11: Replication Visibility

If replication exists, lag must be understood:

```sql
SELECT state, sent_lsn, replay_lsn, replay_lag
FROM pg_stat_replication;
```

- This view shows how far standbys lag behind.
- Replay lag reflects how stale read replicas are.
- This helps DBAs decide whether replicas are safe for reads.

<br>
<details>
<summary><mark><b>Explained with scenarios!</b></mark></summary>
<br>

**What is pg_stat_replication?**
- shows the status of replication connection to standby or logical replicas.
- It tells you who is replicating from this server, how far behind they are, and whether replication is healthy (lag, state, sync / async status).
- When we have replication set up (streaming or logical), we must monitor lag to know if replicas are healthy and safe for read traffic.

**Query to run:**
```sql
SELECT 
    application_name,
    client_addr,
    state,                  -- Connection state
    sent_lsn,               -- Where primary has sent WAL
    write_lsn,              -- Where replica has written WAL
    flush_lsn,              -- Where replica has flushed WAL
    replay_lsn,             -- Where replica has applied (replayed) WAL
    write_lag,              -- Time lag for write
    flush_lag,              -- Time lag for flush
    replay_lag              -- Time lag for replay (most important)
FROM pg_stat_replication;
```

<br>
<br>

**All possible scenarios:**
**1. Healthy replica – almost no lagExample:**
**Example:**
```text
state       | streaming
replay_lsn  | 0/12345678
sent_lsn    | 0/12345678
replay_lag  | 00:00:00.012   (12 ms)
```

> Replica is applying changes almost instantly.

**Action:** Safe for read traffic.

<br>
<br>

**2. Small but acceptable replay_lag**
**Example:**
```text
replay_lag  | 00:00:05       (5 seconds)
state       | streaming
```

> Slight delay - normal under moderate load.
**Action:** Fine for most apps. Monitor if it grows.

<br>
<br>

**3. Growing replay_lag:**
**Example:**
```text
replay_lag  | 00:15:30       (15 minutes)
state       | streaming
```

> Replica is applying WAL slower than primary generates it.

**Causes**: Slow disk on replica, heavy queries on replica, network issues, or underpowered replica.
**Action**: Check replica CPU/disk, pause heavy reads, increase `max_wal_senders` or bandwidth.

<br>
<br>

**4. High write/flush_lag but low replay_lag → WAL receive delay**
**Example:**
```text
write_lag   | 00:02:30
replay_lag  | 00:00:02
```

> Replica receives WAL slowly (network/disk), but applies fast once received.
**Action**: Check network latency, increase `wal_receiver_timeout`, or improve storage.

<br>
<br>

**5. State = startup → Replica still catching up after restart**
**Example:**
```text
state       | startup
replay_lag  | 02:30:00       (2.5 hours)
```

> Replica is syncing from base backup or catching up after downtime.

<br>
<br>

**6. State = catchup → Replica catching up after lag**
**Example:**
```text
state       | catchup
replay_lag  | 00:45:00
```

> Replica fell behind and is now trying to catch up.
**Action**: Reduce load on replica, fix root cause (disk/network).

<br>
<br>

**7. No rows or disconnected → Replication broken**
**Example:** Query returns nothing.
> Replica not connected.

**Causes:** Network down, auth issue, primary crashed, max_wal_senders exhausted.
**Action:** Checks logs, pg_is_wal_replay_paused, restart replica recovery, or fix connection.

<br>
<br>

**8. Logcal replication log (pg_stat_subscription)**
For logical: check pg_stat_subscription on subscriber:
```sql
SELECT subname, received_lsn, last_msg_send_time, last_msg_receipt_time
FROM pg_stat_subscription;
```

> Large time gap = logical lag (conflicts, slow apply).

- `replay_lag` < 1-2 seconds → Excellent.
- 5-30 seconds → Acceptable for many apps.
- . > 1 minute → Investigate urgently.

</details>
<br>

---

<br>
<br>

## Step 12: Archiving and Recovery Safety

For PITR setups, archiving must work:

```sql
SELECT archived_count, failed_count
FROM pg_stat_archiver;
```

- Failures here mean backups are incomplete.
- This view is critical for recovery confidence.

<br>
<details>
<summary><mark><b>Explained with scenarios!</b></mark></summary>
<br>

**What is pg_stat_archiver?**
- shows WAL archiving activity.
- It tells you whether WAL files are being archived successfully, how many succeeded or failed, and helps detect archiving problems that can break PITR and backups.
- For Point-in-Time Recovery (PITR) to work, WAL archiving must succeed every time. Even one failed archive can create a gap and make recovery impossible past that point.

**Query to run:**
```sql
SELECT 
    archived_count,      -- Successful WAL archives
    failed_count,        -- Failed WAL archives
    last_archived_wal,   -- Last successfully archived WAL file
    last_archived_time,  -- When it happened
    last_failed_wal,     -- Last WAL that failed to archive
    last_failed_time     -- When the failure occurred
FROM pg_stat_archiver;
```

<br>
<br>

**All possible scenarios:**
**1. High `archived_count` + `failed_count = 0` -> perfect archiving (healthy)**
**Example:**
```text
archived_count    | 1,850,420
failed_count      | 0
last_archived_time| 2025-12-24 10:45:32
```

> Every WAL file archived successfully.
**Action**: PITR is fully safe. Keep monitoring – this is ideal!

<br>
<br>

**2. Low or zero `archived_count` + `failed_count = 0` -> Archiving not active or very quite**
**Example:**
```text
archived_count    | 15
failed_count      | 0
```

> Almost no archiving happening.
**Causes**: archive_mode = off, wrong archive_command, or read-only/low-write system.
**Action**: If PITR needed, enable and test archiving.

<br>
<br>

**3. Any failed_count > 0 -> critical failure - recovery gap**
**Example:**
```text
archived_count    | 1,850,000
failed_count      | 26
last_failed_wal   | 000000010000003A0000009F
last_failed_time  | 2025-12-23 14:22:10
```

> 26 WAL files failed -> huge hole in archive chain -> cannot recover past the first failure.

**Action:** Urgent - check logs, fix `archive_command` (disk full? permissions? network?), re-archive missing files if possible.

<br>
<br>

**4. Rising `failed_count` over time -> ongoing archiving problem:**
**Example:** failed_count increases every hour.
**Causes:** destination full, slow network, script error, permission denied.
**Action:** Fix immediately - monitor logs for archive_command errors.

<br>
<br>

**5. High archived_count + occasional small failed_count (retried successfully)**
**Example:**
```text
archived_count    | 2,100,000
failed_count      | 5
```

> pgsql retries failed archives. If failures are old and no recent ones -> probably recovered.
**Action:** Check `last_failed_time`. If old -> okay now. Still investigate why it happened.

<br>
<br>

**6. last_archived_time is old → Archiving stalled**
**Example:**
```text
last_archived_time| 2025-12-20 08:15:00  (4 days ago)
archived_count    | 950,000
failed_count      | 12,500
```

> archiving completely stuck -> massive gap building.
**Action:** Emergency - restart archiving, fix command, free space.


<br>
<br>

**Quick Monitoring:**
```sql
SELECT 
    archived_count,
    failed_count,
    CASE WHEN failed_count > 0 THEN 'DANGER: Archiving failures!' ELSE 'OK' END AS status,
    last_archived_time
FROM pg_stat_archiver;
```

- failed_count = 0 → Safe for PITR.
- failed_count > 0 → Recovery confidence lost – backups incomplete.

- Check `pg_stat_archiver` daily in PITR setups.
- Even one failure breaks the chain → no recovery past that point.
- Test `archive_command` manually: `sh -c 'your_archive_command'`

**Summary:** `pg_stat_archiver` is your PITR lifeline: `failed_count > 0` = incomplete backups and broken recovery. Zero failures = full confidence in restores!


</details>
<br>

---

<br>
<br>

## Step 13: Statistics Are Not Instant

- Statistics update lazily.
- A running query does not update counters until it finishes or goes idle.
- Within a transaction, statistics appear frozen.
- This behavior is intentional and prevents inconsistent reads of stats.
- A DBA must always remember this delay when diagnosing live systems.

---

<br>
<br>

## Step 14: Resetting Statistics Carefully

Statistics can be reset:

```sql
SELECT pg_stat_reset();
```

- This clears history.
- But it also resets autovacuum decision counters.
- After a reset, ANALYZE should be run to rebuild planner visibility.
- Statistics reset is a scalpel, not a hammer.

---

<br>
<br>

## Final Understanding Through This Flow

- PGSQL statistics are PGSQL’s self-awareness.
- They explain what is happening now, what happened recently, and where pressure exists.
- A DBA does not guess performance problems. A DBA reads `pg_stat_*`.
- If statistics make sense, performance stops being mysterious.
