# PostgreSQL Backup & Restore – SQL Dump, Restore, pg_dumpall, and Large DB Handling (Practical Flow)

<br>
<br>

## Starting Point: What a SQL Dump Actually Is

- We can use a SQL dump as a backup file that contains lots of SQL commands. When we need to restore, we can run those commands again – they will rebuild our database tables, objects, and load all the data back. We should think of it as taking a simple snapshot of our database in the form of easy-to-read SQL instructions.

- When we want to create a dump, we can let PostgreSQL connect to our running database, read everything – the structure and the data – and turn it into those SQL commands. Later, if we need to, we can run this file on another PostgreSQL server to recreate the exact same database state.

<br>
<br>

## The Basic Dump Command (pg_dump)

When we need to create a simple backup of one database, we can run a command like:
```bash
pg_dump mydb > mydb.sql
```

- Here, `pg_dump` connects to our database called `"mydb"` and writes all the SQL commands into the file `mydb.sql`.
- We should remember that `pg_dump` is just a normal PostgreSQL client tool. It connects using the standard PostgreSQL protocol, so we can run it from any machine that has network access to the database server.
Since `pg_dump` only reads data (it doesn't change anything), we need proper permissions. To dump an entire database, we usually need to connect as a superuser or a role that can access every object.

If we want to control which server to connect to, we can use options like this:
```bash
pg_dump -h server_ip -p 5432 -U postgres mydb > mydb.sql
```

- `-h` lets us specify the host (server IP or name).
- `-p` lets us choose the port (default is 5432).
- `-U` lets us pick which PostgreSQL user to connect as.

We can run this safely while the database is online – no downtime needed!

<br>
<br>

## Why SQL Dumps Are So Useful

- We can restore SQL dumps into newer PostgreSQL versions without any issues. We can also move databases easily across different operating systems and even CPU architectures. Physical backups cannot do that, so we should rely on SQL dumps when we need to upgrade or migrate our database.

- Another important thing we should know: <mark><b>SQL dumps are transaction-consistent</b></mark>. They <mark><b>capture the exact state of our database at the moment the dump starts</b></mark>. While we run the dump, other users can keep working normally – it won't corrupt our backup at all.

<br>
<br>

## Restoring a SQL Dump

We can restore a SQL dump because it's just plain SQL text. To do it, we simply feed the file into the psql tool like this:

```bash
psql -X newdb < mydb.sql
```

Before we run this, we need to make sure the target database already exists. For a clean restore, we should create it from `template0`:

```bash
createdb -T template0 newdb
```

- We can use `-X` to make sure psql ignores our personal configuration files (.psqlrc) and follows only what's in the dump.

- If the dump has objects owned by specific roles/users, we need to ensure those roles already exist in the target server. If they don't, PostgreSQL will give errors about ownership and permissions.

If we want the restore to stop immediately on the first error, we can add:

```bash
psql -X --set ON_ERROR_STOP=on newdb < mydb.sql
```

If we want everything to restore as a single transaction (all or nothing), we can use:

```bash
psql -X -1 newdb < mydb.sql
```

With the single transaction option (`-1` or `--single-transaction`), either the whole dump succeeds perfectly, or nothing gets applied – which helps keep our database consistent!

<br>
<br>

## Restoring Using pg_restore

If the dump file is not SQL text (for example custom format or directory format), the psql approach will not work. In that case, restoration is done using pg_restore:

```
pg_restore -d newdb backup.dump
```

Custom and directory formats offer selective restore. You can choose which tables or schemas to restore.

<br>
<br>

## Dumping Directly Across Servers

Because pg_dump writes to stdout and psql reads from stdin, you can dump and restore between servers in one streaming command:

```
pg_dump -h old_host mydb | psql -X -h new_host mydb
```

This moves database contents from one machine to another without creating intermediate files.

<br>
<br>

## Important Detail About Template Databases

SQL dumps are based on template0. If template1 has custom changes like languages or extensions, they will appear in the dump. When restoring, you must create databases from template0 to avoid duplication problems.

<br>
<br>

## Why ANALYZE Matters After a Restore

After loading data into a fresh database, PostgreSQL has no statistics for the planner. Running ANALYZE fills these statistics:

```
vacuumdb --analyze newdb
```

Without it, queries may perform poorly.

<br>
<br>

## Using pg_dumpall for Full Cluster Backups

pg_dump dumps one database at a time. It does not include global objects like roles and tablespaces.

To capture everything in a PostgreSQL cluster:

```
pg_dumpall > entire_cluster.sql
```

This dump contains:

* roles
* tablespaces
* databases

To restore the cluster:

```
psql -X -f entire_cluster.sql postgres
```

You must restore as a superuser because role and tablespace creation requires superuser authority.

pg_dumpall works by dumping globals first and then invoking pg_dump for each database.

If you only want global objects:

```
pg_dumpall --globals-only > globals.sql
```

<br>
<br>

## Handling Very Large Databases

Large databases create large dump files. Some filesystems do not allow multi‑gigabyte files. PostgreSQL supports multiple techniques to solve this.

### Compression

```
pg_dump mydb | gzip > mydb.sql.gz
```

Restore:

```
gunzip -c mydb.sql.gz | psql newdb
```

### Splitting Output Files

```
pg_dump mydb | split -b 2G - part_
```

Restore:

```
cat part_* | psql newdb
```

### Custom Format Dumps

```
pg_dump -Fc mydb > mydb.dump
```

Restore selectively:

```
pg_restore -d newdb mydb.dump
```

### Parallel Dumps

Parallel dumping speeds up extraction:

```
pg_dump -j 4 -F d -f outdir mydb
```

Parallel restore:

```
p pg_restore -j 4 -d newdb outdir
```

Parallel mode works only with directory format dumps.

<br>
<br>

## What Matters Most in Real Work

A dump and restore workflow must be tested on non‑production servers. Real backups become useful only when you have proven that a restore works. Always monitor:

* dump size
* dump time
* restore time
* disk space for dumps
* user existence before restore

With these commands and flow in hand, PostgreSQL backup and restore becomes predictable rather than mysterious.
